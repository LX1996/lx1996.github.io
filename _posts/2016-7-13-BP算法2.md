---
title: 神经网络的计算
updated: 2016-07-13 22:22
---
假设我们有一个固定样本集 ![image](http://deeplearning.stanford.edu/wiki/images/math/b/4/4/b449e6d375809abbc4097d2c55e9f8c0.png)它包含  m 个样例。我们可以用批量梯度下降法来求解神经网络。具体来讲，对于单个样例(x, y)
其代价函数为：

![image](http://deeplearning.stanford.edu/wiki/images/math/0/2/9/029cdd402b83ee43c7e9a900dccd675a.png)

这是一个（二分之一的）方差代价函数。给定一个包含  m 个样例的数据集，我们可以定义整体代价函数为：

![image](http://deeplearning.stanford.edu/wiki/images/math/4/5/3/4539f5f00edca977011089b902670513.png)

我们的目标是针对参数  W 和  b 来求其函数 J(W,b) 的最小值。为了求解神经网络，我们需要将每一个参数 初始化为一个很小的、接近零的随机值（比如说，使用正态分布 ![image](http://deeplearning.stanford.edu/wiki/images/math/b/1/9/b19e677536c9c7b9da542e4d36c07001.png) 生成的随机值），之后对目标函数使用诸如批量梯度下降法的最优化算法。因为  J(W, b) 是一个非凸函数，梯度下降法很可能会收敛到局部最优解；但是在实际应用中，梯度下降法通常能得到令人满意的结果。最后，需要再次强调的是，要将参数进行随机初始化，而不是全部置为0。如果所有参数都用相同的值作为初始值，那么所有隐藏层单元最终会得到与输入值有关的、相同的函数。随机初始化的目的是使对称失效。

梯度下降法中每一次迭代都按照如下公式对参数  W 和 b 进行更新：

![image](http://deeplearning.stanford.edu/wiki/images/math/6/f/e/6fe7c74511cd6d49a4c9cb6de2afdc33.png)

现在来看一下反向传播算法，它是计算偏导数的一种有效方法。

如何使用反向传播算法来计算![image](http://deeplearning.stanford.edu/wiki/images/math/5/f/b/5fb8e62e296ad365a076617b04d66d03.png)和![image](http://deeplearning.stanford.edu/wiki/images/math/c/a/4/ca49d387f9ead91008f9688b3880e91b.png)一旦我们求出该偏导数，就可以推导出整体代价函数  J(W,b) 的偏导数：
![image](http://deeplearning.stanford.edu/wiki/images/math/9/3/3/93367cceb154c392aa7f3e0f5684a495.png)

反向传播算法的思路如下：给定一个样例 (x,y)，我们首先进行“前向传导”运算，计算出网络中所有的激活值。之后，针对第 l 层的每一个节点 i，我们计算出其“残差”，该残差表明了该节点对最终输出值的残差产生了多少影响。对于最终的输出节点，我们可以直接算出网络产生的激活值与实际值之间的差距。对于隐藏单元我们如何处理呢？我们将基于节点残差的加权平均值计算，这些节点以 ![image](http://deeplearning.stanford.edu/wiki/images/math/c/9/b/c9b144e0a6735fafb01b3615a2a0dc05.png)作为输入。下面将给出反向传导算法的细节：


1.进行前馈传导计算，利用前向传导公式，得到 L_2, L_3,直到输出层 L_n 的激活值。
2.对于输出层的每个输出单元i，我们根据以下公式计算残差：
![image](http://deeplearning.stanford.edu/wiki/images/math/5/7/a/57a203683fc9c009c41ff97c1e1f6f54.png)

注：
![image](http://deeplearning.stanford.edu/wiki/images/math/0/b/0/0b057858cd01020adb2c41cd8a586049.png)

3.对于l = n-1, n-2, n-3的各个层，第l层的第i个节点残差计算方法如下：
![image](http://deeplearning.stanford.edu/wiki/images/math/7/0/1/701c8dc8dbd71013c6a4110a1cb4f6f7.png)

![image](http://deeplearning.stanford.edu/wiki/images/math/2/0/f/20f9979d6a46e7bca83f217bdfead4f0.png)

4.计算我们需要的偏导数，计算方法如下：

![image](http://deeplearning.stanford.edu/wiki/images/math/2/1/d/21db5874b1c1c14bcb675e9961dac9cb.png)

最后，我们用矩阵-向量表示法重写以上算法。

![image](http://deeplearning.stanford.edu/wiki/images/math/c/7/5/c7515c53b59e670ceee277e06c1229cb.png)

那么，反向传播算法可表示为以下几个步骤：
1.进行前馈传导计算，利用前向传导公式，得到 L_2, L_3,直到输出层的激活值。
2.对输出层，计算：

![image](http://deeplearning.stanford.edu/wiki/images/math/0/e/a/0ea6bda6255f544dca0bfa80d622f382.png)


3.对于2到n-1各层，计算

![image](http://deeplearning.stanford.edu/wiki/images/math/7/d/5/7d5660d4a911ecb84113c436f82b1109.png)

4.计算最终所需偏导数

![image](http://deeplearning.stanford.edu/wiki/images/math/5/3/9/5391ac390a4e279ac8a543d4d5498ecc.png)
