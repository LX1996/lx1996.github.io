---
title: 特征选择与特征提取
updated: 2016-07-16 15:30
---


**1.特征选择 V.S 特征提取**

特征选择和特征提取都是DimensionalityReduction的两种方法，针对the curse of dimensionality（维灾难），都可以达到降维的目的。

特征提取（Feature Extraction）: Creatting a subset of new features by combinations of exsiting features. 新特征是原来特征的一个映射。

特征选择(Feature Selection)：choosing a subset of all the features(the ones more informative)。新特征是选来特征的子集

![image](http://my.csdn.net/uploads/201206/20/1340175187_8926.jpg)

**2. 类别可分性判据**

2.1 基于距离的可分性判据

2.1.1 类内距离

类内距离度量的是在特定特征集合X上同类别样本之间的相似程度。
	
2.1.2 类间距离

类间距离衡量的是不同类别样本之间的差异程度
		
2.2 基于散布矩阵的可分性判据

类内散布矩阵
	<img src="http://www.forkosh.com/mathtex.cgi? $S_w=\sum_{i=1}^{c}P_iS_i">

其中 <img src="http://www.forkosh.com/mathtex.cgi? $S_i=\frac{1}{n}\sum_{k=1}^{n_i}(x_k^{i} - u^{i})(x_k^{i} - u^{i})^{T}">

Si 为 第i个类别的类内散布矩阵

Sw 为所有类别的类内散布矩阵

类间散布矩阵
<img src="http://www.forkosh.com/mathtex.cgi? $S_b=\sum_{i=1}^{c}P_i(u^{i} - u)(u^{i} - u)_^{T}">



**2. 主成分分析(Principle Components Analysis,PCA)**

一般来说，随着特征数量的减少，样本中的信息会有所丢失，新的特征对样本的描述也可能会存在一定的误差，PCA就是尽量从减少信息损失的角度来实现特征降维的。

pca 算法步骤：
总结一下PCA的算法步骤：

设有m条n维数据。

1）将原始数据按列组成n行m列矩阵X

2）将X的每一行（代表一个属性字段）进行零均值化，即减去这一行的均值

3）求出协方差矩阵C = 1/m XX'
4）求出协方差矩阵的特征值及对应的特征向量

5）将特征向量按对应特征值大小从上到下按行排列成矩阵，取前k行组成矩阵P

6）Y=PX即为降维到k维后的数据

**d'的选择方法：**
常用的一种d‘参数的选择方法是将所有特征值按照从小到大排序之后计算累加值，以累加值与所有特征值的总和之比不超过95％的原则来选择d’。

**3. 基于Fisher准则的可分性分析(Fisher Discriminant Analysis，FDA)**

PCA的目标是消除特征之间的相关性，而没有考虑样本集中样本的类别属性，因此是一种无监督学习方法。
FDA，有时也被称为线性可分性分析（Linear Discriminant Analysis，LDA），它的出发点是要使得经过降维后的特征能够尽量多的保留类别之间的可分性信息，使得经过降维之后的样本集合具有最大的类别可分性。

FDA 算法步骤：

1）计算各类样本的均值和总体均值

2）计算矩阵<img src="http://www.forkosh.com/mathtex.cgi? $S_w^{-1}S_b">的特征值和特征矢量，特征值从小到大排序

3）选择前d'各特征矢量作为列矢量构成矩阵 E = (e1, e2 ...... ed')

4) d维特征矢量转为d'维特征矢量x'： <img src="http://www.forkosh.com/mathtex.cgi? $x^{'}=E^{T}x">

FDA的相关问题

1）特征矢量之间不具有正交性，新的坐标系不再是直角坐标系，非直角坐标系下仍然能表示每一个特征矢量，只是特征间具有一定的相关性。

2）一般来说训练样本足够多时（n > d),可以保证<img src="http://www.forkosh.com/mathtex.cgi? $S_w">是非奇异的。

3）d'的选择：对于c个类别的样本集来说，矩阵<img src="http://www.forkosh.com/mathtex.cgi? $S_w^{-1}S_b">之多之存在c-1个特征值大于等于0，其他的d-c＋1个特征值均为0。当类别数较大时，可以根据情况选择d' < c - 1，但当类别数较少时只能选择d'为<img src="http://www.forkosh.com/mathtex.cgi? $S_w^{-1}S_b">非0特征根的个数。

