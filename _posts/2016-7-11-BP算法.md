---
title: BP算法
updated: 2016-07-11 20:22
---
**LMS算法**

感知机算法的损失函数是误分类点到 Target 平面的总距离，直观解释如下：当一个实例点被误分，则调整 w， b 的值，使得分离超平面向该误分类点的一侧移动，以减少该误分类点与超平面的距离。

假设我们的线性模型是这样的：

![image](http://images.cnitblog.com/blog/533521/201306/03233254-3176ade5771e42988b03c18fc7c1d331.png)

在上面这个模型中，用公式可以表达成：

<img src="http://www.forkosh.com/mathtex.cgi? $h_\theta%20{(x)}%20=%20\theta%20^Tx=\theta%20_{1}x_1+\theta%20_2x_2%20+\theta%20_0">

如何判断模型的好坏呢？损失函数定义为输出值 h(x) 与目标值 y 之间的“二乘”：
<img src="http://images.cnitblog.com/blog/533521/201306/03233308-b261f125994e482f9f88050d943ccf2b.png">

对偏导进行求解，可以得到：
<img src="http://images.cnitblog.com/blog/533521/201306/03233317-775d2307dc7c46b8a5dfba716c2a4bd5.png">

如果要利用 gradient descent 的方法找到一个好的模型，即一个合适的 theta 向量，迭代的公式为：

<img src="http://images.cnitblog.com/blog/533521/201306/03233327-4fce87a4944d41748e22ccd713cb0f3a.png">

所以，对于一个第 i 个单独的训练样本来说，我们的第 j 个权重更新公式是：

<img src="http://latex.codecogs.com/gif.latex?\theta%20_j=\theta%20_j+\alpha%20(y^{(i)}%20-%20h_\theta%20(x^{(i)}))%20x_j^{i}">


这个更新的规则也叫做 Widrow-Hoff learning rule， 从上到下推导下来只有几步，没有什么高深的理论，但是，仔细观察上面的公式，就可以发现几个 natural and intuitive 的特性。

首先，权重的更新是跟 y - h(x) 相关的，如果训练样本中预测值与 y 非常接近，表示模型趋于完善，权重改变小。反之，如果预测值与 y 距离比较远，说明模型比较差，这时候权重变化就比较大了。

权重的变化还与 xi 也就是输入节点的值相关。也就是说，在同一次 train 中，由于 y - h(x) 相同， 细线上的变化与相应的输入节点 x 的大小是成正比的(参考最上面的模型图）。这中间体现的直观印象就是：残差的影响是按照 xi 分配到权重上去滴，这里的残差就是 h(x) - y。

**MLP 与 BP 算法**

 logistic regression 本质上是线性分类器，只不过是在线性变换后通过 sigmoid 函数作非线性变换。而神经网络 MLP 还要在这个基础上加上一个新的nonlinear function, 为了讨论方便，这里的 nonlinear function 都用 sigmoid 函数，并且损失函数忽略 regulization term， 那么， MLP 的结构就可以用下面这个图来表示：
 ![image](http://images.cnitblog.com/blog/533521/201306/03233344-3eedac97c3be4799b3653fcd7bb7828a.png)
 
 现在我们要利用 LMS 中的想法来对这个网络进行训练。

假设在某一个时刻，输入节点接受一个输入， MLP 将数据从左到右处理得到输出，这时候产生了残差。在第一小节中，我们知道， LMS 残差等于 h(x) - y。 MLP 的最后一层和 LMS 线性分类器非常相似，我们不妨先把最后一层的权重更新问题解决掉。在这里输出节点由于增加了一个非线性函数，残差的值比 LMS 的残差多了一个求导 (实际上是数学上 chain rule 的推导)：

<img src="http://deeplearning.stanford.edu/wiki/images/math/5/7/a/57a203683fc9c009c41ff97c1e1f6f54.png">

得到残差，根据之前猜想出来的规律( - -!), 残差的影响是按照左侧输入节点的 a 值大小按比例分配到权重上去的，所以呢，就可以得到:

<img src = "http://deeplearning.stanford.edu/wiki/images/math/2/1/d/21db5874b1c1c14bcb675e9961dac9cb.png">

如果乘以一个 learning rate, 这就是最后一层的权重更新值。

我们在想，要是能得到中间隐层节点上的残差，问题就分解成几个我们刚刚解决的问题。关键是：中间隐层的残差怎么算？

实际上就是按照权重与残差的乘积返回到上一层。完了之后还要乘以非线性函数的导数( again it can be explained by chain rule)：

<img src = "http://deeplearning.stanford.edu/wiki/images/math/2/0/f/20f9979d6a46e7bca83f217bdfead4f0.png">

得到隐层的残差，我们又可以得到前一层权重的更新值了。这样问题就一步一步解决了。

最后我们发现，其实咱们不用逐层将求残差和权值更新交替进行，可以这样:

1.先从右到左把每个节点的残差求出来(数学上表现为反向传导过程)

2.然后再求权重的更新值

3.更新权重

用一张粗略的静态图表示残差的反向传播：

![image](http://images.cnitblog.com/blog/533521/201306/03233410-bc4e0b43225e4d3ab7a61a4c2849b29d.png)

 用一张动态图表示前向（FP）和后向（BP）传播的全过程：
 
 ![image](http://images.cnitblog.com/blog/533521/201306/08135834-8e9b8ff2212545c0aeb1d68103ef3d64.gif)
 
 
总结起来，一次 BP 权值调整的过程是这样的：

1.输入向量从输入节点依次向后传播，我们可以计算出 the activation of all the hidden and output units

2.计算每个输出节点的残差

3.输出节点的残差依次向前传播，由此可以求得各个隐层的残差

4.由隐层的残差可以求得隐层左侧权重的更新
在 BP 算法中，我们 backpropagation 的残差其实是对偏导或者导数的计算。当然，在数学上，你也可以对每个权重利用 chain rule 进行偏导计算，得到每个权重的 update, 但是这样显然有很多重复计算， 而 BP 在一定程度上解决了这个问题。归根结底， backpropagation technique provide a computationally efficient method for evaluating such derivatives.



